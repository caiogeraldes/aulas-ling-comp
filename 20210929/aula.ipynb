{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LC - Pós - Corpus 2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw41n6KrvUOd"
      },
      "source": [
        "# Riqueza Vocabular\n",
        "\n",
        "Trata-se da proporção entre o vocabulário e o número de palavras presentes num texto, ou seja: \n",
        "\n",
        "> $Riqueza~Vocabular = \\frac{types}{tokens}$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Exercício**\n",
        "\n",
        "Calcule a riqueza vocabular do livro \"O Guarani\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9p0R4LYqwg6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA8CHnd8vdju"
      },
      "source": [
        "# Palavras vazias (stopwords)\n",
        "\n",
        "Existe uma classe de palavras que se caracteriza por sua fraca contribuição semântica e alta frequência em textos e discursos. Entre elas estão as preposições, pronomes, artigos etc. São chamadas *palavras vazias* por oposição às *palavras plenas* (aquelas que compõem categorias semânticas. \n",
        "\n",
        "Em muitas tarefas do processamento linguístico envolvendo semântica, costuma-se deixar de fora essa classe de palavras.\n",
        "\n",
        "O principal critério computacional de detecção de palavras vazias (*stopwords*, em inglês) é frequencial. Dado um grande conjunto de textos (com 1 milhão de palavras ou mais), as $n$ primeiras palavras serão vazias. O valor de $n$ varia conforme a extensão do corpus, mas, com 1 M de palavras, estará situado em torno de 100.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Exercício**\n",
        "\n",
        "Crie um dicionário de ocorrências de \"O Guarani\". Mostre as 100 palavras mais frequentes. Use o método Counter.most_commmon(100) para isso.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx8CE8Z6e-15"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxeH4qkDvh0z"
      },
      "source": [
        "# Hápax Legômena\n",
        "\n",
        "Os hápax legômena (singular: legômenon) são palavras que aparecem no corpus uma única vez.\n",
        "\n",
        "Há muitos usos práticos para a detecção e, muitas vezes, para eliminação dessas palavras na análise computacional. Elas podem indicar erros, nomes próprios, valores numéricos etc. Além disso, do ponto de vista da distribuição da massa probabilística, os hápax legômenon tendem a ocupar cerca de 50% de todo o vocabulário.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Exercício\n",
        "\n",
        "1. Crie uma lista dos hápax legômena do livro \"O Guarani\".\n",
        "2. Quantos são?\n",
        "3. Observe 100 desses hápax. Tente responder à questão: o que caracteriza tais ocorrências únicas?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UddfZW6fB_9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciYPMgRcfETi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBZZS6vxfELG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUsemykzs8hp"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# NLTK\n",
        "\n",
        "O NLTK (Natural Language ToolKit) é um módulo externo que pode ser importado para uso em seus scripts em Python. \n",
        "\n",
        "O módulo oferece um conjunto de classes úteis para o processamento linguístico: tokenizador, stemizador e muitos outros. \n",
        "\n",
        "Oferece, também, acesso fácil a um conjunto de corpora integrados ao módulo. \n",
        "\n",
        "O site do módulo (https://www.nltk.org/) traz, ainda, um excelente livro sobre processamento linguístico baseado em seus recursos: \"NLP with Python\". O livro pode ser lido online aqui: https://www.nltk.org/book/\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGW1qdQA3pGu",
        "outputId": "3ba51754-9b6b-43ff-e65e-c38d9123fcd2"
      },
      "source": [
        "# Daqui por diante, vamos usar esta string multi-linhas nos exemplos:\n",
        "\n",
        "texto = '''O rato roeu a roupa do Rei de Roma.\\n Em seguida, jogou-se n'água do Mar Tirreno... \\t Calcula-se que o prejuízo tenha sido alto: R$ 10.000,00.\n",
        "'''\n",
        "\n",
        "print(texto)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O rato roeu a roupa do Rei de Roma.\n",
            " Em seguida, jogou-se n'água do Mar Tirreno... \t Calcula-se que o prejuízo tenha sido alto: R$ 10.000,00.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRTyp5Q-ue2h"
      },
      "source": [
        "# Tokenizador\n",
        "\n",
        "O NLTK traz tokenizadores de diferentes tipos e para muitas línguas. Aqui, vamos usar o tokenizador para o português."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knv9CANA8fHD",
        "outputId": "d8660c7a-4c59-4b36-9aa8-93dc2c6589e9"
      },
      "source": [
        "# Importação do módulo e dos recursos necessários\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUZdobR48wae",
        "outputId": "ac5acd03-d5b4-4772-ad7e-a1d52fc798fc"
      },
      "source": [
        "# Tokenização de palavras e pontuação\n",
        "from nltk import tokenize \n",
        "\n",
        "tokens = tokenize.word_tokenize(texto, language='portuguese')\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O', 'rato', 'roeu', 'a', 'roupa', 'do', 'Rei', 'de', 'Roma', '.', 'Em', 'seguida', ',', 'jogou-se', \"n'água\", 'do', 'Mar', 'Tirreno', '...', 'Calcula-se', 'que', 'o', 'prejuízo', 'tenha', 'sido', 'alto', ':', 'R', '$', '10.000,00', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDYznYU4uzgP"
      },
      "source": [
        "# Palavras vazias (stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNYukNDa-uMm",
        "outputId": "da97e5ed-b1ca-4b3f-c595-9caaef1971cd"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "stops = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "len(stops)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "204"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P21rjvA5fZg7"
      },
      "source": [
        "**Exercício**\n",
        "\n",
        "Crie uma função para mostrar a lista de tokens (do tópico de Tokenização, acima) sem as palavras vazias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xneaqRVdfu6Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul5lmmQ7vIU0"
      },
      "source": [
        "# Stemização"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjLrWGPeswSx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d24290-267f-407a-f117-1def8d86f651"
      },
      "source": [
        "# Carregamento do stemizador\n",
        "nltk.download('rslp')\n",
        "raiz = nltk.stem.RSLPStemmer().stem\n",
        "\n",
        "print(raiz('nada'))\n",
        "print(raiz('nadinha'))\n",
        "print(raiz('nadabóbora'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n",
            "nad\n",
            "nad\n",
            "nadabób\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk1xfLywfzD9"
      },
      "source": [
        "**Exercício**\n",
        "\n",
        "Crie uma função para stemizar palavras. Aplique-a à lista de tokens sem palavras vazias (do exercício anterior)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t2yrz7zf-fS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
